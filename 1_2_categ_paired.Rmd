---
title: "Categorical variables"
subtitle: "Paired samples"
author: "Albert Cobos"
output: 
  ioslides_presentation:
    widescreen: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, 
                      warning=FALSE, 
                      error = TRUE,
                      comment = NA,
                      message = FALSE)
```

## Paired data

A study comparing the physician's opinion and recommendations made by a clinical decision support system (`CDSS`), on the need of lipid lowering drugs (LLD) in 500 hypercholesterolemic patients

```{r}
d <- rio::import("https://raw.githubusercontent.com/acobos/Datasets/master/OptimCare.txt")
head(d)
```

Both the `CDSS` and the `Physician` assessed the need of LLD (as `Yes` or `No`)


## Paired data

Two questions of possible interest:

1. Are the CDSS and the physician equally prone to prescribe LLD?

2. How well do they agree?

\ 

Answers:

1. McNemar's test

2. Cohen's kappa (measure of agreement)


## McNemar's test 

Compares the two table margins, i.e., the *marginal homogeneity*

```{r}
cdss_phy <- mosaic::tally(CDSS ~ Physician, data=d)  
addmargins(cdss_phy)   # to show margins (Sum) of the contingency table 
```

Proportion of `Yes`:

- Physician: $\quad$ 287 / 500 = 0.574
- CDSS:  $\qquad$ 234 / 500 = 0.468

## McNemar's test 

```{r}
mcnemar.test(cdss_phy)         
```

\
\
The low p-value provides evidence of marginal *heterogeneity*, which implies a different probability of prescribing LLD by the CDSS and the physician.

\

The test may NOT be valid if there very few discrepant results (e.g. < 10).


## Agreement: observed and expected by chance

```{r echo=FALSE}
library(epiR)
round(chisq.test(cdss_phy)$expected,1) -> e

list(observed = cdss_phy,
     expected = e)

round(epi.kappa(cdss_phy)$prop,3) -> ag

```

- $P_o$ =  (`r cdss_phy[1,1]` + `r cdss_phy[2,2]`) / 500 = `r ag$obs`
- $P_e$ = (`r e[1,1]` + `r e[2,2]`) / 500 = `r ag$exp`

## Cohen's kappa

$\kappa = \frac{P_o - P_e}{1 - P_e} \quad$ = 
(`r ag$obs` - `r ag$exp`) / (1 - `r ag$exp`) = `r round(epi.kappa(cdss_phy)$kappa[1],3)`

```{r echo=FALSE, fig.width=8}
library(epiR)
# epi.kappa(x)$prop
# epi.kappa(x)$kappa

epi.kappa(cdss_phy)$prop -> k

barplot(k$obs, beside = FALSE, 
        xlim = 0:1, ylim = 0:1, width=.2, las=1)

text("observed", x= 0.32, y = k$obs, adj=0)
arrows(0, k$obs, 0.3, k$obs, code = 0, lty=2)
arrows(0.48, k$obs, 1, k$obs, code = 0, lty=2)
# abline(h = k$obs, lty=2)


text("expected", x= 0.32, y = k$exp, adj=0)
arrows(0, k$exp, 0.3, k$exp, code = 0, lty=2)
arrows(0.48, k$exp, 1, k$exp, code = 0, lty=2)
# abline(h = k$exp, lty=2)

arrows(0, 1, 1, 1, code = 0, lty=2)
# abline(h = 1, lty=2)

arrows(0.7, k$exp, 0.7, k$obs, code = 3, length=0.15)
arrows(0.75, k$exp, 0.75, 1, code = 3, length=0.15)

axis(side=4, at = c(k$exp,1), labels = c(0,1), las=1)

text("Po-Pe", x= 0.7, y = k$exp + (k$obs-k$exp)/2, pos=2)
text("1-Pe", x= 0.75, y = k$exp + (k$obs-k$exp)/2, pos=4)

```

## Cohen's kappa {.smaller}

Function `epi.kappa()` in package `epiR`

```{r}
res <- epiR::epi.kappa(cdss_phy)    
names(res)
res$prop.agree         # proportions of observed and expected agreement
res$kappa              # kappa (est) and CI (lower, upper)
```


## Exercise 

- BTA test vs cytology in diagnosis or follow-up of bladder cancer patients 

- Reference standard: cystoscopy

```{r echo=FALSE}
downloadthis::download_file(
  path = "./data/BTA.xls",
  button_label = "Download data",
  button_type = "primary",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)

```

\

```{r}
d <- rio::import("data/bta.xls")
head(d)
```

## Exercise (cont.)

Using cystoscopy as the reference standard:

1. Compute sensitivity and specificity of the BTA test 

2. Compute sensitivity and specificity of cytology 

3. Do these procedures have equal sensitivity?

4. Do they have equal specificity?

5. To what extend the results of both procedures agree?

